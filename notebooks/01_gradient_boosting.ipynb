{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "This notebook implements a gradient boosting baseline for the NeurIPS polymer competition. The approach uses AutoGluon to automatically select and tune ensemble methods on molecular features extracted from SMILES strings. Due to a small dataset, only gradient boosting (and limited Random Forest) models are used.\n",
    "\n",
    "The baseline combines:\n",
    "- RDKit molecular descriptors\n",
    "- Morgan fingerprints (radius=2, 256 bits)\n",
    "- MACCS keys fingerprints\n",
    "\n",
    "Features are then pruned to avoid overfitting on a small datasets by reducing dimensionality."
   ],
   "id": "4f7a37c0bbe7374e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles, Descriptors\n",
    "from rdkit.Chem.rdMolDescriptors import GetMACCSKeysFingerprint\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error"
   ],
   "id": "b623b9f64ba3b234",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_smile_canonical(smile: str) -> Union[str, np.nan]:\n",
    "    \"\"\"Makes a SMILE string canonical to prevent duplicates.\n",
    "\n",
    "    Returns:\n",
    "        The canonical SMILE string or NaN if parsing error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mol = MolFromSmiles(smile)\n",
    "        return MolToSmiles(mol, canonical=True)\n",
    "    except:\n",
    "        return np.nan"
   ],
   "id": "cd5c5d100349cd7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_file(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Sanitises data from a CSV file.\"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    data[\"SMILES\"] = data[\"SMILES\"].apply(make_smile_canonical)\n",
    "\n",
    "    return (data.dropna(subset=[\"SMILES\"])\n",
    "            .drop_duplicates(subset=[\"SMILES\"]))"
   ],
   "id": "8d9e1076714321b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_train_data() -> pd.DataFrame:\n",
    "    \"\"\"Loads all competition input data and computes weights.\n",
    "\n",
    "    Prioritises train.csv properties over supplementary data properties when merging in\n",
    "    supplementary data.\n",
    "    \"\"\"\n",
    "    primary_data = read_file(\"../data/train.csv\")\n",
    "    supplement_paths = [\n",
    "        \"../data/train_supplement/dataset1.csv\",\n",
    "        \"../data/train_supplement/dataset3.csv\",\n",
    "        \"../data/train_supplement/dataset4.csv\",\n",
    "    ]\n",
    "\n",
    "    for path in supplement_paths:\n",
    "        sec_data = read_file(path)\n",
    "        if \"TC_mean\" in sec_data.columns:\n",
    "            sec_data = sec_data.rename(columns={\"TC_mean\": \"Tc\"})\n",
    "\n",
    "        primary_data = primary_data.merge(sec_data, on=\"SMILES\", how=\"outer\", suffixes=(\"\", \"_supp\"))\n",
    "\n",
    "        for col in sec_data.columns:\n",
    "            if col != \"SMILES\" and col in primary_data.columns:\n",
    "                supp_col = f\"{col}_supp\"\n",
    "                if supp_col not in primary_data.columns:\n",
    "                    continue\n",
    "                primary_data[col] = primary_data[col].fillna(primary_data[supp_col])\n",
    "\n",
    "    supp_columns = filter(lambda col_name: \"_supp\" in col_name, primary_data.columns)\n",
    "    clean_data = primary_data.drop(columns=list(supp_columns) + [\"id\"])\n",
    "    \n",
    "    return clean_data\n",
    "\n"
   ],
   "id": "186df037ffc8b737",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feature extraction rationale\n",
    "\n",
    "Features need to balance predictive power and dimensionality due to small datasets for some properties (~600). This is why, e.g., Mordred descriptors were not included.\n",
    "\n",
    "- RDKit descriptors capture fundamental molecular properties (MW, logP, etc.)\n",
    "- Morgan fingerprints encode local structural patterns around atoms\n",
    "- MACCS keys express interpretable structural patterns\n",
    "\n",
    "Features with low-variance and highly correlation to other features (but one of a group) were removed to prevent overfitting."
   ],
   "id": "6dcd6c2a2cc92636"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "morgan_gen = GetMorganGenerator(radius=2, fpSize=256)\n",
    "\n",
    "def extract_features(smile: str) -> pd.Series:\n",
    "    \"\"\"Generates baseline features from a SMILE string.\n",
    "\n",
    "    This includes RDKit descriptors, the morgan fingerprint (set up with length=256 and radius=2),\n",
    "    and the MACCSKeysFingerprint. This setup was chosen to avoid overfitting due to too many features.\n",
    "    \"\"\"\n",
    "    mol = MolFromSmiles(smile)\n",
    "\n",
    "    desc = [desc_fn(mol) for name, desc_fn in Descriptors.descList]\n",
    "    maccs = GetMACCSKeysFingerprint(mol)\n",
    "    morgan = morgan_gen.GetFingerprint(mol)\n",
    "    features = pd.Series(desc + list(maccs) + list(morgan))\n",
    "\n",
    "    # XGBoost does not perform optimally with infinite-like values.\n",
    "    return (features.mask(features.abs() > 1e10, np.nan)\n",
    "            .replace([np.inf, -np.inf], np.nan))"
   ],
   "id": "618ee7e6e9f39455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_feature_mask(features: pd.DataFrame, variance_threshold = 0.01, correlation_threshold = 0.95) -> pd.Series:\n",
    "    \"\"\"Gets a mask that removes redundant features.\n",
    "\n",
    "    This includes features with low variance and all but one of highly correlated features.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(True, index=features.columns)\n",
    "    mask &= features.var() >= variance_threshold\n",
    "\n",
    "    remaining_features = features.loc[:, mask]\n",
    "\n",
    "    corr_matrix = remaining_features.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1)\n",
    "    )\n",
    "\n",
    "    # Find highly correlated pairs and remove one from each pair\n",
    "    high_corr_pairs = np.where(upper_triangle > correlation_threshold)\n",
    "    features_to_drop = set(corr_matrix.columns[high_corr_pairs[1]])\n",
    "\n",
    "    for feature in features_to_drop:\n",
    "        mask[feature] = False\n",
    "\n",
    "    return mask"
   ],
   "id": "7af1598d4a93d0b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_ag(target: str, data: pd.DataFrame, group: str, quality: str, time_limit: int) -> TabularPredictor:\n",
    "    \"\"\"Trains an AutoGluon predictor adjusted for small datasets.\n",
    "\n",
    "    Args:\n",
    "        group: the folder where the model is saved (inside models/)\n",
    "    \"\"\"\n",
    "    other_targets = [col for col in [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"] if col != target]\n",
    "    valid_rows = data[~data[target].isna()]\n",
    "    is_small_dataset = valid_rows.shape[0] < 1000\n",
    "\n",
    "    predictor = TabularPredictor(\n",
    "        label=target,\n",
    "        eval_metric=\"mean_absolute_error\",\n",
    "        path=f\"../models/{group}/{target}\"\n",
    "    )\n",
    "\n",
    "    predictor.fit(\n",
    "        valid_rows.drop(columns=other_targets),\n",
    "        presets=quality,\n",
    "        time_limit=time_limit,\n",
    "        excluded_model_types=[\"NN\", \"FASTAI\"] if is_small_dataset else [],\n",
    "        num_stack_levels=0 if is_small_dataset else 1,\n",
    "    )\n",
    "\n",
    "    return predictor"
   ],
   "id": "6d6d9aaa20bfcc42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation and training\n",
    "\n",
    "5-fold CV mean absolute error was used to evaluate models as it is a good proxy for the metric used by the competition. Out-of-fold predictions are captured which could assist meta-learning later (though this could lead to overfitting due to very small samples).\n",
    "\n",
    "Once a satisfactory setup was reached, the model was retrained on all data. Breaking convention is justified by the small dataset size and need to maximise training data for final predictions."
   ],
   "id": "5c2ca301633a6bd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data = load_train_data()\n",
    "\n",
    "features = train_data[\"SMILES\"].apply(extract_features)\n",
    "mask = get_feature_mask(features)\n",
    "\n",
    "train_data = (pd.concat([train_data, features.loc[:, mask]], axis=1)\n",
    "              .drop(columns=[\"SMILES\"]))"
   ],
   "id": "aef85bdc2d18a206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "oof_preds = pd.DataFrame(index=train_data.index)\n",
    "targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "\n",
    "for target in targets:\n",
    "    k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in tqdm(\n",
    "        k_fold.split(train_data),\n",
    "        desc=f\"Evaluating {target} AutoGluon model\"\n",
    "    ):\n",
    "        train_subset, eval_subset = train_data.iloc[train_index], train_data.iloc[test_index]\n",
    "        predictor = train_ag(target, train_subset, \"autogluon-eval\", \"medium_quality\", 300)\n",
    "        predictions = predictor.predict(eval_subset.drop(columns=targets))\n",
    "\n",
    "        oof_preds.loc[eval_subset.index, target] = predictions\n",
    "\n",
    "    mae = mean_absolute_error(oof_preds[target], train_data[target])\n",
    "    print(f\"Finished evaluating {target}: mae={mae}\")"
   ],
   "id": "ba264d1381a993c5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
