{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This notebook extends the baseline gradient boosting approach with more comprehensive feature engineering. Multiple fingerprint types, descriptors, 3D conformational features, and transformer embeddings are explored to improve predictive performance.\n",
    "\n",
    "Target-specific feature combinations are optimised through systematic evaluation, balancing feature diversity with overfitting prevention on the limited dataset."
   ],
   "id": "7507ac5bb3bd746e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import importnb\n",
    "import joblib\n",
    "from rdkit.Chem import MolFromSmiles, Descriptors\n",
    "from rdkit.Chem.rdFingerprintGenerator import *\n",
    "from rdkit.Chem.rdMolDescriptors import GetMACCSKeysFingerprint\n",
    "from rdkit.Chem.GraphDescriptors import *\n",
    "from rdkit.Chem.Crippen import MolLogP, MolMR\n",
    "from rdkit.Chem.Lipinski import NumHDonors, NumHAcceptors\n",
    "from rdkit.Chem.rdMolDescriptors import CalcNumRotatableBonds, CalcTPSA\n",
    "from rdkit.Chem import AllChem, Descriptors3D\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold"
   ],
   "id": "761ea851e267bf02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "grad_boosting = importnb.Notebook.load_file(\"01_gradient_boosting.ipynb\", include_non_defs=False)",
   "id": "96125d958c10293f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def validate_features(feature: pd.Series) -> pd.Series:\n    \"\"\"Replaces infinite and infinite-like values with NaN.\"\"\"\n    feature = pd.to_numeric(feature, errors='coerce')\n    feature = feature.replace([np.inf, -np.inf], np.nan)\n    \n    with np.errstate(over='ignore'):\n        feature = feature.mask(feature.abs() > 1e10, np.nan)\n    \n    return feature",
   "id": "4552202e14196523",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def create_fingerprint_getter(generator):\n    \"\"\"Creates a callable that returns a fingerprint from a SMILES string.\n\n    Args:\n        generator: RDKit fingerprint generator with GetFingerprint method.\n    \"\"\"\n    def get_fingerprint(smile: str) -> pd.Series:\n        \"\"\"Returns validated fingerprint from the configured generator.\"\"\"\n        mol = MolFromSmiles(smile)\n        fingerprint = pd.Series(list(generator.GetFingerprint(mol)))\n        return validate_features(fingerprint)\n\n    return get_fingerprint",
   "id": "6e0c01a98cf275cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "get_morgan_2_fp = create_fingerprint_getter(GetMorganGenerator(radius=2, fpSize=256))\n",
    "get_morgan_5_fp = create_fingerprint_getter(GetMorganGenerator(radius=5, fpSize=256))\n",
    "get_rdkit_fp = create_fingerprint_getter(GetRDKitFPGenerator(fpSize=256))\n",
    "get_atom_pair_fp = create_fingerprint_getter(GetAtomPairGenerator(fpSize=256))\n",
    "get_topological_torsion_fp = create_fingerprint_getter(GetTopologicalTorsionGenerator(fpSize=256))"
   ],
   "id": "67d8c78657311d2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def get_maccs_keys_fp(smiles_string: str) -> pd.Series:\n    \"\"\"Extracts MACCS keys fingerprint from SMILES string.\"\"\"\n\n    mol = MolFromSmiles(smiles_string)\n    fingerprint = pd.Series(list(GetMACCSKeysFingerprint(mol)))\n\n    return validate_features(fingerprint)",
   "id": "bb72695bd8be6c3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def get_rdkit_descriptors(smiles_string: str) -> pd.Series:\n    \"\"\"Computes all RDKit molecular descriptors.\"\"\"\n    mol = MolFromSmiles(smiles_string)\n    features = pd.Series([desc_fn(mol) for name, desc_fn in Descriptors.descList]) # type: ignore\n\n    return validate_features(features)",
   "id": "8508dc4bcc7e9083",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_rdkit_3d_features(smiles_string: str) -> pd.Series:\n",
    "    \"\"\"Computes 3D geometric descriptors from optimised molecular conformation.\n",
    "    \n",
    "    Embeds molecule in 3D space and optimises geometry before computing\n",
    "    shape-based descriptors like asphericity and radius of gyration.\n",
    "    \"\"\"\n",
    "    smiles_processed = smiles_string.replace('*', '[H]')\n",
    "    mol = MolFromSmiles(smiles_processed)\n",
    "\n",
    "    try:\n",
    "        if mol is None:\n",
    "            raise ValueError(\"Failed to parse molecule.\")\n",
    "        mol = Chem.AddHs(mol)\n",
    "        result = AllChem.EmbedMolecule(mol, randomSeed=42)  # type: ignore\n",
    "\n",
    "        if result != 0:\n",
    "            raise ValueError(\"Embedding failed\")\n",
    "        try:\n",
    "            AllChem.MMFFOptimizeMolecule(mol)  # type: ignore\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return validate_features(pd.Series({\n",
    "            \"asphericity\": Descriptors3D.Asphericity(mol),  # type: ignore\n",
    "            \"eccentricity\": Descriptors3D.Eccentricity(mol),  # type: ignore\n",
    "            \"inertial_shape_factor\": Descriptors3D.InertialShapeFactor(mol),  # type: ignore\n",
    "            \"radius_of_gyration\": Descriptors3D.RadiusOfGyration(mol),  # type: ignore\n",
    "            \"spherocity_index\": Descriptors3D.SpherocityIndex(mol)  # type: ignore\n",
    "        }))\n",
    "    except ValueError:\n",
    "        return pd.Series({\n",
    "            \"asphericity\": np.nan,\n",
    "            \"eccentricity\": np.nan,\n",
    "            \"inertial_shape_factor\": np.nan,\n",
    "            \"radius_of_gyration\": np.nan,\n",
    "            \"spherocity_index\": np.nan\n",
    "        })"
   ],
   "id": "c159342c8e70b156",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def mol_to_networkx(mol):\n    \"\"\"Converts RDKit molecule to NetworkX graph representation.\"\"\"\n    G = nx.Graph()\n\n    for atom in mol.GetAtoms():\n        G.add_node(\n            atom.GetIdx(),\n            atomic_num=atom.GetAtomicNum(),\n            formal_charge=atom.GetFormalCharge(),\n            hybridization=atom.GetHybridization()\n        )\n\n    for bond in mol.GetBonds():\n        G.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx(), bond_type=bond.GetBondType())\n\n    return G",
   "id": "4449a39ab155325a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def safe_diameter(G):\n    \"\"\"Calculates graph diameter, handling disconnected graphs gracefully.\"\"\"\n    try:\n        if nx.is_connected(G):\n            return nx.diameter(G)\n        else:\n            # Return diameter of largest connected component\n            largest_cc = max(nx.connected_components(G), key=len)\n            subgraph = G.subgraph(largest_cc)\n\n            return nx.diameter(subgraph) if len(largest_cc) > 1 else 0\n    except:\n        return np.nan",
   "id": "a31f8aa2db28db99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def safe_radius(G):\n    \"\"\"Calculates graph radius, handling disconnected graphs gracefully.\"\"\"\n    try:\n        if nx.is_connected(G):\n            return nx.radius(G)\n        else:\n            # Return radius of largest connected component\n            largest_cc = max(nx.connected_components(G), key=len)\n            subgraph = G.subgraph(largest_cc)\n\n            return nx.radius(subgraph) if len(largest_cc) > 1 else 0\n    except:\n        return np.nan",
   "id": "a7fc299cd538a096",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def safe_average_clustering(G):\n    \"\"\"Calculates average clustering coefficient with error handling.\"\"\"\n    try:\n        return nx.average_clustering(G) if G.number_of_nodes() > 0 else 0\n    except:\n        return np.nan",
   "id": "fffcb7723fafa4b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def safe_avg_centrality(G, centrality_func):\n    \"\"\"Calculates average centrality measure with error handling.\n    \n    Args:\n        centrality_func: NetworkX centrality function to apply.\n    \"\"\"\n    try:\n        if G.number_of_nodes() == 0:\n            return 0\n        centrality_dict = centrality_func(G)\n\n        return np.mean(list(centrality_dict.values()))\n    except:\n        return np.nan",
   "id": "62435934d62e92f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def calculate_randic_index(mol):\n    \"\"\"Calculates RandiÄ‡ connectivity index for molecular graph.\"\"\"\n    try:\n        randic = 0.0\n        for bond in mol.GetBonds():\n            atom1 = bond.GetBeginAtom()\n            atom2 = bond.GetEndAtom()\n            deg1 = atom1.GetDegree()\n            deg2 = atom2.GetDegree()\n            if deg1 > 0 and deg2 > 0:\n                randic += 1.0 / np.sqrt(deg1 * deg2)\n\n        return float(randic)\n    except:\n        return np.nan",
   "id": "7ec236b7566c4379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def get_graph_features(smiles_string: str) -> pd.Series:\n    \"\"\"Extracts comprehensive graph-based molecular features.\n    \n    Computes connectivity indices, topological descriptors, ring properties,\n    and physicochemical properties from molecular graph structure.\n    \"\"\"\n    mol = MolFromSmiles(smiles_string)\n\n    features = pd.Series({\n        \"kappa1\": Kappa1(mol),\n        \"chi0v\": Chi0v(mol),\n        \"chi1v\": Chi1v(mol),\n        \"chi0n\": Chi0n(mol),\n        'chi1n': Chi1n(mol),\n        \n        \"hallkieralpha\": HallKierAlpha(mol),\n        \"balabanj\": BalabanJ(mol),\n        \n        \"num_heavy_atoms\": mol.GetNumHeavyAtoms(),\n        \n        \"num_aromatic_rings\": len([x for x in mol.GetRingInfo().AtomRings() if all(mol.GetAtomWithIdx(i).GetIsAromatic() for i in x)]),\n        \"num_aliphatic_rings\": len([x for x in mol.GetRingInfo().AtomRings() if not all(mol.GetAtomWithIdx(i).GetIsAromatic() for i in x)]),\n        \"num_rings\": len(mol.GetRingInfo().AtomRings()),\n        \"largest_ring_size\": max([len(x) for x in mol.GetRingInfo().AtomRings()]) if mol.GetRingInfo().AtomRings() else 0,\n        \n        \"num_hbd\": NumHDonors(mol),\n        \"num_hba\": NumHAcceptors(mol),\n        \"num_rotatable_bonds\": CalcNumRotatableBonds(mol),\n        \"tpsa\": CalcTPSA(mol),\n        \"logp\": MolLogP(mol),\n        \"molmr\": MolMR(mol),\n        \n        \"randic_index\": calculate_randic_index(mol)\n    })\n\n    return validate_features(features)",
   "id": "8117ca3eb9499080",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def get_nx_graph_features(smiles_string: str) -> pd.Series:\n    \"\"\"Extracts NetworkX graph topology features from molecular structure.\"\"\"\n    mol = MolFromSmiles(smiles_string)\n    G = mol_to_networkx(mol)\n\n    features = pd.Series({\n        \"diameter\": safe_diameter(G),\n        \"average_clustering\": safe_average_clustering(G),\n        \"density\": nx.density(G) if G.number_of_nodes() > 0 else 0,\n        \"avg_degree_centrality\": safe_avg_centrality(G, nx.degree_centrality),\n    })\n\n    return validate_features(features)",
   "id": "1cefdf6238a11ff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def mean_pooling(model_output, attention_mask):\n    \"\"\"Applies attention-masked mean pooling to transformer outputs.\n    \n    Implementation from Hugging Face model documentation.\n    \n    Args:\n        attention_mask: Binary mask indicating valid tokens.\n    \"\"\"\n    token_embeddings = model_output[0]\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)",
   "id": "607fdaff7e64e32c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def set_up_model(model_name: str):\n    \"\"\"Loads pre-trained tokeniser and model from Hugging Face.\"\"\"\n    tokeniser = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\n    return tokeniser, model",
   "id": "a6193dd0b6d37b78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_batches(smiles_list, model_name: str, pooling_fn, device: str = \"mps\", batch_size: int = 32):\n",
    "    \"\"\"Processes SMILES strings through transformer model in batches.\n",
    "    \n",
    "    Args:\n",
    "        pooling_fn: Function to pool token embeddings into sentence embeddings.\n",
    "        batch_size: Number of samples per batch.\n",
    "    \"\"\"\n",
    "    tokeniser, model = set_up_model(model_name)\n",
    "    model = model.to(device)\n",
    "    tokens = tokeniser(smiles_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    smile_dataset = TensorDataset(tokens[\"input_ids\"], tokens[\"attention_mask\"])\n",
    "    dataloader = DataLoader(smile_dataset, batch_size=batch_size)\n",
    "    \n",
    "    embeddings = []\n",
    "    for input_ids, attention_mask in tqdm(dataloader, desc=model_name):\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embedding = pooling_fn(outputs, attention_mask)\n",
    "            embeddings.append(embedding.cpu())\n",
    "    \n",
    "    return pd.DataFrame(torch.cat(embeddings).numpy())"
   ],
   "id": "d24e794bcafd4385",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_embedding_fingerprints(smiles_strings: pd.Series) -> tuple:\n",
    "    \"\"\"Generates transformer-based molecular embeddings using polyBERT, ChemBERTa, and MoLFormer.\"\"\"\n",
    "    smiles_list = smiles_strings.tolist()\n",
    "\n",
    "    polyBERT_fingerprints = process_batches(\n",
    "        smiles_list,\n",
    "        \"kuelumbus/polyBERT\",\n",
    "        mean_pooling,\n",
    "    )\n",
    "\n",
    "    chemBERTa_fingerprints = process_batches(\n",
    "        smiles_list,\n",
    "        \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "        lambda outputs, mask: outputs.last_hidden_state[:, 0, :]\n",
    "    )\n",
    "\n",
    "    MoLFormer_fingerprints = process_batches(\n",
    "        smiles_list,\n",
    "        \"ibm/MoLFormer-XL-both-10pct\",\n",
    "        lambda outputs, mask: outputs.pooler_output,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    return polyBERT_fingerprints, chemBERTa_fingerprints, MoLFormer_fingerprints"
   ],
   "id": "d7e4fe05ff37c60e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data = grad_boosting.load_train_data()\n",
    "smiles = train_data[\"SMILES\"]"
   ],
   "id": "77aa8f4b5d11136f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rdkit_descriptors = smiles.apply(get_rdkit_descriptors)\n",
    "\n",
    "nx_graph_features = smiles.apply(get_nx_graph_features)\n",
    "rdkit_3d_features = smiles.apply(get_rdkit_3d_features)\n",
    "graph_features = smiles.apply(get_graph_features)\n",
    "\n",
    "morgan_2_fps = smiles.apply(get_morgan_2_fp)\n",
    "morgan_5_fps = smiles.apply(get_morgan_5_fp)\n",
    "rdkit_fps = smiles.apply(get_rdkit_fp)\n",
    "atom_pair_fps = smiles.apply(get_atom_pair_fp)\n",
    "topological_torsion_fps = smiles.apply(get_topological_torsion_fp)\n",
    "maccs_fps = smiles.apply(get_maccs_keys_fp)\n",
    "\n",
    "polyBERT_fps, chemBERTa_fps, MoLFormer_fps = get_embedding_fingerprints(smiles)"
   ],
   "id": "5c50d5e805ced83c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def combine_features(train_data: pd.DataFrame, *feature_sets) -> tuple:\n    \"\"\"Combines multiple feature sets with variance and correlation filtering.\n    \n    Args:\n        *feature_sets: Variable number of feature DataFrames to combine.\n    \"\"\"\n    all_features = []\n    for i, feature_set in enumerate(feature_sets):\n        feature_set_renamed = feature_set.add_suffix(f'_set{i}')\n        all_features.append(feature_set_renamed)\n\n    features = pd.concat(all_features, axis=1)\n    mask = grad_boosting.get_feature_mask(features)\n    train_data = (pd.concat([train_data, features.loc[:, mask]], axis=1)\n                  .drop(columns=[\"SMILES\"]))\n\n    return train_data, mask",
   "id": "4f396fd1820ad4d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feature Selection\n",
    "\n",
    "The set of features to be used for each property was determined in two parts: first, descriptors and fingerprints were tested in isolation and compared. With sample sizes as small as ~600, it's unlikely that complex relationships would be discovered, so exploring combinations is less beneficial.\n",
    "\n",
    "Then, smaller features sets, which underperformed in step 1, were added to provide additional context, relying on domain knowledge. For example, Glass transition temperature (Tg) was likely to benefit from 3D features."
   ],
   "id": "9fdce91782da96f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tg_feats, tg_mask = combine_features(train_data, rdkit_descriptors, maccs_fps, morgan_2_fps, rdkit_3d_features)\n",
    "ffv_feats, ffv_mask = combine_features(train_data, rdkit_descriptors, maccs_fps, morgan_2_fps, graph_features)\n",
    "tc_feats, tc_mask = combine_features(train_data, rdkit_descriptors, morgan_5_fps, atom_pair_fps)\n",
    "density_feats, density_mask = combine_features(train_data, rdkit_descriptors, maccs_fps, morgan_2_fps, rdkit_3d_features)\n",
    "rg_feats, rg_mask = combine_features(train_data, rdkit_descriptors, maccs_fps, topological_torsion_fps, rdkit_3d_features)"
   ],
   "id": "5370c329f9c49b28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "oof_preds = pd.DataFrame(index=train_data.index)\n",
    "targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "features = [tg_feats, ffv_feats, tc_feats, density_feats, rg_feats]\n",
    "\n",
    "for target, feats in zip(targets, features):\n",
    "    k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tqdm(\n",
    "        k_fold.split(train_data),\n",
    "        desc=f\"Evaluating {target} AutoGluon model\"\n",
    "    )):\n",
    "        train_subset, eval_subset = feats.iloc[train_index], feats.iloc[test_index]\n",
    "        predictor = grad_boosting.train_ag(target, train_subset, \"autogluon-eval\", \"good\", 500)\n",
    "        predictions = predictor.predict(eval_subset.drop(columns=targets))\n",
    "\n",
    "        oof_preds.loc[eval_subset.index, target] = predictions\n",
    "\n",
    "oof_preds.to_csv(\"ag_preds.csv\")"
   ],
   "id": "eecc707ab275020d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grad_boosting.train_ag(\"Tg\", tg_feats)\n",
    "grad_boosting.train_ag(\"FFV\", ffv_feats)\n",
    "grad_boosting.train_ag(\"Tc\", tc_feats)\n",
    "grad_boosting.train_ag(\"Density\", density_feats)\n",
    "grad_boosting.train_ag(\"Rg\", rg_feats)\n",
    "\n",
    "masks = {\n",
    "    \"Tg\": tg_mask,\n",
    "    \"FFV\": ffv_mask,\n",
    "    \"Tc\": tc_mask,\n",
    "    \"Density\": density_mask,\n",
    "    \"Rg\": rg_mask\n",
    "}\n",
    "joblib.dump(masks, \"../models/autogluon/masks.pkl\")"
   ],
   "id": "e860c2039e2d46da",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
